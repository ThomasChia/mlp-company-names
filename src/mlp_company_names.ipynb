{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of names from data\n",
    "data = pd.read_csv('../data/companies_sorted.csv')\n",
    "names = data['name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut data to first million names to reduce training time - not looking for the most accurate anyway.\n",
    "names = names[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary of characters and index mapping - to and from integers\n",
    "# Involves making sure all the names are strings (as some are floats) and removing any names with non-alphanumeric characters\n",
    "# (as some of these companies are from other countries and have non-English characters in their names).\n",
    "# Maybe we should limit to US and UK companies only as the names are likely to be more similar and contain English words - extension.\n",
    "names = [str(name) for name in names]\n",
    "pattern = re.compile(r'^[a-zA-Z\\d]+$')\n",
    "filtered_names = [name for name in names if pattern.match(name)]\n",
    "chars = sorted(list(set(''.join(filtered_names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {char: i+1 for i, char in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: char for char, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(stoi) == len(chars) + 1, \"Error: Vocabulary size does not match character set size\"\n",
    "assert len(stoi) == len(itos), \"Error: Indexes to characters and characters to indexes do not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibm\n",
      "... --> i\n",
      "..i --> b\n",
      ".ib --> m\n",
      "ibm --> .\n",
      "accenture\n",
      "... --> a\n",
      "..a --> c\n",
      ".ac --> c\n",
      "acc --> e\n",
      "cce --> n\n",
      "cen --> t\n",
      "ent --> u\n",
      "ntu --> r\n",
      "tur --> e\n",
      "ure --> .\n",
      "ey\n",
      "... --> e\n",
      "..e --> y\n",
      ".ey --> .\n"
     ]
    }
   ],
   "source": [
    "# Create x and y datasets from list of company names.\n",
    "# I want two lists of lists, one for x and one for y\n",
    "# x contain the first x letters of each name, each letter will be its own element of a list\n",
    "# y contains the next letter\n",
    "block_size = 3\n",
    "X, Y = [], []\n",
    "for name in filtered_names[:3]:\n",
    "    print(name)\n",
    "    context = [0] * block_size\n",
    "    for char in name + '.':\n",
    "        ix = stoi[char]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join([itos[i] for i in context]), '-->', char)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 3, 10])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the embeddings for the characters.\n",
    "# The embeddings will be a feature vector of length 10 for each character.\n",
    "# The table of embeddings will be a matrix of size (vocab_size, embedding_size).\n",
    "\n",
    "embedding_size = 10\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "# Createa randn tensor of size (vocab_size, embedding_size) as the embeddings lookup table.\n",
    "C = torch.randn(vocab_size, embedding_size)\n",
    "# Apply these embeddings to the input data by using pytorch indexing.\n",
    "embeddings = C[X]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the first hidden layer.\n",
    "# The input of this layer will be each of the characters in the context, input as their feature vectors (embeddings),\n",
    "# the block_size (3) * embedding_size (10) = 30.\n",
    "# The bias will be a vector of size 50, as the hidden layer has 50 outputs.\n",
    "# In the paper, the hidden layer either has 0, 50, or 100 outputs.\n",
    "\n",
    "hidden_size = 50\n",
    "\n",
    "W1 = torch.randn(block_size * embedding_size, hidden_size)\n",
    "b1 = torch.randn(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0953, -1.1443,  0.6619, -1.3265, -0.7251,  1.8243, -0.9412, -1.1199,\n",
       "         -1.5182,  0.7003],\n",
       "        [-0.0953, -1.1443,  0.6619, -1.3265, -0.7251,  1.8243, -0.9412, -1.1199,\n",
       "         -1.5182,  0.7003],\n",
       "        [-0.0953, -1.1443,  0.6619, -1.3265, -0.7251,  1.8243, -0.9412, -1.1199,\n",
       "         -1.5182,  0.7003]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 50])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].flatten() @ W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
